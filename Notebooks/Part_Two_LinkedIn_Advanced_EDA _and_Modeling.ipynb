{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8daed7e",
   "metadata": {},
   "source": [
    "# Part II - Advanced EDA and Baseline Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb245c0",
   "metadata": {},
   "source": [
    "This is the second notebook of capstone project. It continues from the first notebook, \"LinkedIn_Preliminary_EDA\". In the previous notebook, we explored the LinkedIn Job Postings data from 2023, and were motivated by the following question:\n",
    "\n",
    "> How do different job attributes affect the number of views for a job posting? Which job attribute should companies emphasize to raise more views? Which job attribute do job applicants prioritize when searching for a job?\n",
    "\n",
    "We cleaned the data and conducted some basic EDA to begin answering tis quesiton.\n",
    "\n",
    "Now we will conduct further analysis through advanced EDA and preprocessing to prepare a dataframe for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9b6ae6",
   "metadata": {},
   "source": [
    "**Author: JJ Park**\n",
    "\n",
    "**Date: 26/03/2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8586e5d9",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb724c",
   "metadata": {},
   "source": [
    "Dataset has been sourced from: https://www.kaggle.com/datasets/arshkon/linkedin-job-postings/data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9dd3a",
   "metadata": {},
   "source": [
    "<a id= 'table' ></a>\n",
    "### Table of Contents\n",
    "\n",
    "1. [Data Dictionary](#Dictionary)  \n",
    "2. [Data Loading](#Load)\n",
    "3. [Data Preprocessing - Feature Engineering](#Feature)\n",
    "4. [Basic Model Preprocessing](#Preprocess)\n",
    "5. [Modeling - Linear Regression](#Linear)\n",
    "6. [Modeling - Random Forest Regressor](#Random)\n",
    "7. [Modeling - XG Boost](#XG)\n",
    "7. [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e9ef9",
   "metadata": {},
   "source": [
    "<a id = 'Dictionary'><a/>   \n",
    "## Data Dictionary\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560ddc7",
   "metadata": {},
   "source": [
    "1. `job_id`: The job ID as defined by LinkedIn (https://www.linkedin.com/jobs/view/{ job_id })\n",
    "2. `company_id`: Identifier for the company associated with the job posting (maps to companies.csv)\n",
    "3. `title`: Job title\n",
    "4. `description`: Job description\n",
    "5. `max_salary`: Maximum salary\n",
    "6. `med_salary`: Median salary\n",
    "7. `min_salary`: Minimum salary\n",
    "8. `pay_period`: Pay period for salary (Hourly, Monthly, Yearly)\n",
    "9. `formatted_work_type`: Type of work (Fulltime, Parttime, Contract)\n",
    "10. `location`: Job location\n",
    "11. `applies`: Number of applications that have been submitted\n",
    "12. `original_listed_time`: Original time the job was listed\n",
    "13. `remote_allowed`: Whether job permits remote work\n",
    "14. `views`: Number of times the job posting has been viewed\n",
    "15. `job_posting_url`: URL to the job posting on a platform\n",
    "16. `application_url`: URL where applications can be submitted\n",
    "17. `application_type`: Type of application process (offsite, complex/simple onsite)\n",
    "18. `expiry`: Expiration date or time for the job listing\n",
    "19. `closed_time`: Time to close job listing\n",
    "20. `formatted_experience_level`: Job experience level (entry, associate, executive, etc)\n",
    "21. `skills_desc`: Description detailing required skills for job\n",
    "22. `listed_time`: Time when the job was listed\n",
    "23. `posting_domain`: Domain of the website with application\n",
    "24. `sponsored`: Whether the job listing is sponsored or promoted\n",
    "25. `work_type`: Type of work associated with the job\n",
    "26. `currency`: Currency in which the salary is provided\n",
    "27. `compensation_type`: Type of compensation for the job\n",
    "28. `scraped`: Has been scraped by `details_retriever`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf043e",
   "metadata": {},
   "source": [
    "<a id = 'Load'><a/>   \n",
    "## Data Loading\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c3a785",
   "metadata": {},
   "source": [
    "Let's begin by importing all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e479d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efefde85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d1058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the cleaned data from previous notebook\n",
    "df = pd.read_csv('../Data/clean_linkedin_job_posting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc05ec",
   "metadata": {},
   "source": [
    "Let's check the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ecc65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>company_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>med_salary</th>\n",
       "      <th>pay_period</th>\n",
       "      <th>formatted_work_type</th>\n",
       "      <th>location</th>\n",
       "      <th>applies</th>\n",
       "      <th>original_listed_time</th>\n",
       "      <th>remote_allowed</th>\n",
       "      <th>views</th>\n",
       "      <th>application_type</th>\n",
       "      <th>expiry</th>\n",
       "      <th>formatted_experience_level</th>\n",
       "      <th>listed_time</th>\n",
       "      <th>sponsored</th>\n",
       "      <th>reposted</th>\n",
       "      <th>skills_present</th>\n",
       "      <th>application_portal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3757940104</td>\n",
       "      <td>553718.0</td>\n",
       "      <td>Hearing Care Provider</td>\n",
       "      <td>Overview\\n\\nHearingLife is a national hearing ...</td>\n",
       "      <td>5250.00</td>\n",
       "      <td>MONTHLY</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Little River, SC</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023-11-04 05:26:40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>OffsiteApply</td>\n",
       "      <td>2023-12-04 03:53:20</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>2023-11-04 05:26:40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3757940025</td>\n",
       "      <td>2192142.0</td>\n",
       "      <td>Shipping &amp; Receiving Associate 2nd shift (Beav...</td>\n",
       "      <td>Metalcraft of Mayville\\nMetalcraft of Mayville...</td>\n",
       "      <td>73028.00</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Beaver Dam, WI</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023-11-04 02:40:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>OffsiteApply</td>\n",
       "      <td>2023-12-04 03:53:20</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>2023-11-04 02:40:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3757938019</td>\n",
       "      <td>474443.0</td>\n",
       "      <td>Manager, Engineering</td>\n",
       "      <td>\\nThe TSUBAKI name is synonymous with excellen...</td>\n",
       "      <td>73028.00</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Bessemer, AL</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023-11-04 02:40:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>OffsiteApply</td>\n",
       "      <td>2023-12-04 03:53:20</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>2023-11-04 02:40:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3757938018</td>\n",
       "      <td>18213359.0</td>\n",
       "      <td>Cook</td>\n",
       "      <td>descriptionTitle\\n\\n Looking for a great oppor...</td>\n",
       "      <td>22.27</td>\n",
       "      <td>HOURLY</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Aliso Viejo, CA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023-11-04 02:40:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OffsiteApply</td>\n",
       "      <td>2023-12-04 03:53:20</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>2023-11-04 02:40:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3757937095</td>\n",
       "      <td>437225.0</td>\n",
       "      <td>Principal Cloud Security Architect (Remote)</td>\n",
       "      <td>Job Summary\\nAt iHerb, we are on a mission to ...</td>\n",
       "      <td>240895.00</td>\n",
       "      <td>YEARLY</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>United States</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023-11-02 20:06:40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>OffsiteApply</td>\n",
       "      <td>2023-12-04 03:53:20</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>2023-11-04 05:26:40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       job_id  company_id                                              title  \\\n",
       "0  3757940104    553718.0                              Hearing Care Provider   \n",
       "1  3757940025   2192142.0  Shipping & Receiving Associate 2nd shift (Beav...   \n",
       "2  3757938019    474443.0                               Manager, Engineering   \n",
       "3  3757938018  18213359.0                                               Cook   \n",
       "4  3757937095    437225.0        Principal Cloud Security Architect (Remote)   \n",
       "\n",
       "                                         description  med_salary  \\\n",
       "0  Overview\\n\\nHearingLife is a national hearing ...     5250.00   \n",
       "1  Metalcraft of Mayville\\nMetalcraft of Mayville...    73028.00   \n",
       "2  \\nThe TSUBAKI name is synonymous with excellen...    73028.00   \n",
       "3  descriptionTitle\\n\\n Looking for a great oppor...       22.27   \n",
       "4  Job Summary\\nAt iHerb, we are on a mission to ...   240895.00   \n",
       "\n",
       "      pay_period formatted_work_type          location  applies  \\\n",
       "0        MONTHLY           Full-time  Little River, SC      5.0   \n",
       "1  Not Specified           Full-time    Beaver Dam, WI      5.0   \n",
       "2  Not Specified           Full-time      Bessemer, AL      5.0   \n",
       "3         HOURLY           Full-time   Aliso Viejo, CA      5.0   \n",
       "4         YEARLY           Full-time     United States      5.0   \n",
       "\n",
       "  original_listed_time  remote_allowed  views application_type  \\\n",
       "0  2023-11-04 05:26:40             0.0    9.0     OffsiteApply   \n",
       "1  2023-11-04 02:40:00             0.0   16.0     OffsiteApply   \n",
       "2  2023-11-04 02:40:00             0.0   16.0     OffsiteApply   \n",
       "3  2023-11-04 02:40:00             0.0    1.0     OffsiteApply   \n",
       "4  2023-11-02 20:06:40             1.0   16.0     OffsiteApply   \n",
       "\n",
       "                expiry formatted_experience_level          listed_time  \\\n",
       "0  2023-12-04 03:53:20                Entry level  2023-11-04 05:26:40   \n",
       "1  2023-12-04 03:53:20              Not Specified  2023-11-04 02:40:00   \n",
       "2  2023-12-04 03:53:20              Not Specified  2023-11-04 02:40:00   \n",
       "3  2023-12-04 03:53:20                Entry level  2023-11-04 02:40:00   \n",
       "4  2023-12-04 03:53:20           Mid-Senior level  2023-11-04 05:26:40   \n",
       "\n",
       "   sponsored  reposted  skills_present  application_portal  \n",
       "0          0         0               0                   1  \n",
       "1          0         0               0                   1  \n",
       "2          0         0               1                   1  \n",
       "3          0         0               0                   1  \n",
       "4          0         1               0                   1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "873887bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33246, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the number of rows and columns in the cleaned dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77450be0",
   "metadata": {},
   "source": [
    "Let's ensure that the data is actually clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ce89f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33246 entries, 0 to 33245\n",
      "Data columns (total 20 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   job_id                      33246 non-null  int64  \n",
      " 1   company_id                  33246 non-null  float64\n",
      " 2   title                       33246 non-null  object \n",
      " 3   description                 33246 non-null  object \n",
      " 4   med_salary                  33246 non-null  float64\n",
      " 5   pay_period                  33246 non-null  object \n",
      " 6   formatted_work_type         33246 non-null  object \n",
      " 7   location                    33246 non-null  object \n",
      " 8   applies                     33246 non-null  float64\n",
      " 9   original_listed_time        33246 non-null  object \n",
      " 10  remote_allowed              33246 non-null  float64\n",
      " 11  views                       33246 non-null  float64\n",
      " 12  application_type            33246 non-null  object \n",
      " 13  expiry                      33246 non-null  object \n",
      " 14  formatted_experience_level  33246 non-null  object \n",
      " 15  listed_time                 33246 non-null  object \n",
      " 16  sponsored                   33246 non-null  int64  \n",
      " 17  reposted                    33246 non-null  int64  \n",
      " 18  skills_present              33246 non-null  int64  \n",
      " 19  application_portal          33246 non-null  int64  \n",
      "dtypes: float64(5), int64(5), object(10)\n",
      "memory usage: 5.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8865a37e",
   "metadata": {},
   "source": [
    "We can observe the following:\n",
    "- We have 33246 rows with 20 columns. There are 10 numerical and 10 categorical variables. However, the `original_listed_time`, `expiry` and `listed_time` columns have been converted into object datatype from datetime. We will convert them back into datetime in the future analysis.\n",
    "- No null values present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f3048",
   "metadata": {},
   "source": [
    "Let's ensure that there is genuinely no null data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36868711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_id                        0\n",
       "company_id                    0\n",
       "title                         0\n",
       "description                   0\n",
       "med_salary                    0\n",
       "pay_period                    0\n",
       "formatted_work_type           0\n",
       "location                      0\n",
       "applies                       0\n",
       "original_listed_time          0\n",
       "remote_allowed                0\n",
       "views                         0\n",
       "application_type              0\n",
       "expiry                        0\n",
       "formatted_experience_level    0\n",
       "listed_time                   0\n",
       "sponsored                     0\n",
       "reposted                      0\n",
       "skills_present                0\n",
       "application_portal            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ae3be",
   "metadata": {},
   "source": [
    "We see that the dataset is clean with no missing values as we addressed them in our previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc70ef90",
   "metadata": {},
   "source": [
    "Let's also check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "141d87cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking duplicate rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170955d6",
   "metadata": {},
   "source": [
    "There are no duplicates either. Now we can conduct some advanced EDA and data preprocessing for modeling. Let's start by looking at some categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e208529",
   "metadata": {},
   "source": [
    "<a id = 'Feature'><a/>   \n",
    "## Data Preprocessing - Feature Engineering\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d955243",
   "metadata": {},
   "source": [
    "First, let's separate variables by their respective data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12e44117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'description', 'pay_period', 'formatted_work_type', 'location',\n",
       "       'original_listed_time', 'application_type', 'expiry',\n",
       "       'formatted_experience_level', 'listed_time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting all the columns with 'object' datatype\n",
    "df.select_dtypes([\"object\"]).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e687393",
   "metadata": {},
   "source": [
    "There are 10 categorical variables in the dataset. Let's check for numerical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f869d805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['job_id', 'company_id', 'med_salary', 'applies', 'remote_allowed',\n",
       "       'views', 'sponsored', 'reposted', 'skills_present',\n",
       "       'application_portal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting all the columns with 'float' and 'int' datatypes\n",
    "df.select_dtypes([\"float\", \"int\"]).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203f4c1",
   "metadata": {},
   "source": [
    "There are 10 numerical variables in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642bce4e",
   "metadata": {},
   "source": [
    "As part of the advanced EDA, we will conduct some feature engineering to convert all the categorical variables into numerical variables. We will leave out the `title` and `description` columns for now as we plan on conducting various text analysis tasks to extract insights and patterns from the text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257a018",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68feb80a",
   "metadata": {},
   "source": [
    "First, let's take a look at `application_type` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32f89e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "application_type\n",
       "OffsiteApply          20104\n",
       "ComplexOnsiteApply    10723\n",
       "SimpleOnsiteApply      2419\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['application_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f663bf00",
   "metadata": {},
   "source": [
    "As we can see from above, there are three categories. For the simplicity, we will combine the \"ComplexOnsiteApply\" and \"SimpleOnsiteApply\" categories into a single category. Essentially, we are creating a binary column where \"OffsiteApply\" will be represented by the entries of 0, and \"OnsiteApply\" will be represented by the entries of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a0fa56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the 'application_type' column into a binary column\n",
    "df['application_method'] = df['application_type'].map({'OffsiteApply': 0, 'ComplexOnsiteApply': 1, 'SimpleOnsiteApply': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bea92a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "application_method\n",
       "0    20104\n",
       "1    13142\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "df['application_method'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324acb4d",
   "metadata": {},
   "source": [
    "About 60 percent of the applications were offsite submissions and 40 percent were onsite submissions. Now drop the non binarized `application_type` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57671d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33246 entries, 0 to 33245\n",
      "Data columns (total 20 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   job_id                      33246 non-null  int64  \n",
      " 1   company_id                  33246 non-null  float64\n",
      " 2   title                       33246 non-null  object \n",
      " 3   description                 33246 non-null  object \n",
      " 4   med_salary                  33246 non-null  float64\n",
      " 5   pay_period                  33246 non-null  object \n",
      " 6   formatted_work_type         33246 non-null  object \n",
      " 7   location                    33246 non-null  object \n",
      " 8   applies                     33246 non-null  float64\n",
      " 9   original_listed_time        33246 non-null  object \n",
      " 10  remote_allowed              33246 non-null  float64\n",
      " 11  views                       33246 non-null  float64\n",
      " 12  expiry                      33246 non-null  object \n",
      " 13  formatted_experience_level  33246 non-null  object \n",
      " 14  listed_time                 33246 non-null  object \n",
      " 15  sponsored                   33246 non-null  int64  \n",
      " 16  reposted                    33246 non-null  int64  \n",
      " 17  skills_present              33246 non-null  int64  \n",
      " 18  application_portal          33246 non-null  int64  \n",
      " 19  application_method          33246 non-null  int64  \n",
      "dtypes: float64(5), int64(6), object(9)\n",
      "memory usage: 5.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns='application_type', inplace=True)\n",
    "\n",
    "# Sanity Check\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4992cd",
   "metadata": {},
   "source": [
    "Next we will look at `location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76e80ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location\n",
       "United States         2341\n",
       "New York, NY           818\n",
       "Chicago, IL            534\n",
       "Houston, TX            444\n",
       "Dallas, TX             383\n",
       "                      ... \n",
       "Edmonds, WA              1\n",
       "Crawfordsville, IN       1\n",
       "Winter Park, CO          1\n",
       "Claremont, NC            1\n",
       "Fergus Falls, MN         1\n",
       "Name: count, Length: 4621, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605049c1",
   "metadata": {},
   "source": [
    "As we can see, majority of the job postings were published from the United States, specified by popular cities within the United States. Our task is to extract country information to minimize the number of categories within the dataset. Let's separate the `location` data into two categories, United States and non-United States. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ff77867",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_usa\n",
       "1    31692\n",
       "0     1554\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column 'is_usa' with binary values\n",
    "def check_usa(location):\n",
    "    if 'United States' in str(location):\n",
    "        return 1\n",
    "    elif ',' in str(location):\n",
    "        return 1  # Assume it's in the USA if there's a comma (likely a city-state pair)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['is_usa'] = df['location'].apply(check_usa)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "df['is_usa'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d5594",
   "metadata": {},
   "source": [
    "Here, we have created a binary column called `is_usa`, which showcases whether the jobs were posted from the United States or not. Let's create a visualization to display the distribution of location, USA vs non-USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b8d933b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "df['is_usa'].value_counts(normalize=True).plot(kind='bar')\n",
    "plt.title('Distribution of Location'.title())\n",
    "plt.xlabel(\"USA vs World\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d046499",
   "metadata": {},
   "source": [
    "As expected, majority of the job postings were from the US (roughly 95 percent), and the rest of the world (5 percent) were responsible for a small portion of job postings. Now we will drop the non-binarized `location` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23cea97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33246 entries, 0 to 33245\n",
      "Data columns (total 20 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   job_id                      33246 non-null  int64  \n",
      " 1   company_id                  33246 non-null  float64\n",
      " 2   title                       33246 non-null  object \n",
      " 3   description                 33246 non-null  object \n",
      " 4   med_salary                  33246 non-null  float64\n",
      " 5   pay_period                  33246 non-null  object \n",
      " 6   formatted_work_type         33246 non-null  object \n",
      " 7   applies                     33246 non-null  float64\n",
      " 8   original_listed_time        33246 non-null  object \n",
      " 9   remote_allowed              33246 non-null  float64\n",
      " 10  views                       33246 non-null  float64\n",
      " 11  expiry                      33246 non-null  object \n",
      " 12  formatted_experience_level  33246 non-null  object \n",
      " 13  listed_time                 33246 non-null  object \n",
      " 14  sponsored                   33246 non-null  int64  \n",
      " 15  reposted                    33246 non-null  int64  \n",
      " 16  skills_present              33246 non-null  int64  \n",
      " 17  application_portal          33246 non-null  int64  \n",
      " 18  application_method          33246 non-null  int64  \n",
      " 19  is_usa                      33246 non-null  int64  \n",
      "dtypes: float64(5), int64(7), object(8)\n",
      "memory usage: 5.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns='location', inplace=True)\n",
    "\n",
    "# Sanity Check\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ecbe1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file in the 'Data' folder\n",
    "df.to_csv(\"../Data/cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b615a",
   "metadata": {},
   "source": [
    "The `location` column has been dropped. We will shift our focus to time variables, including `original_listed_time`, `expiry` and `listed_time`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['original_listed_time'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d68489",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['expiry'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802cefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['listed_time'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4235d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['listed_time'] = pd.to_datetime(df['listed_time'])\n",
    "\n",
    "# Define time periods\n",
    "time_periods = {\n",
    "    'Morning': (pd.Timestamp('06:00:00').time(), pd.Timestamp('11:59:59').time()),\n",
    "    'Afternoon': (pd.Timestamp('12:00:00').time(), pd.Timestamp('17:59:59').time()),\n",
    "    'Evening': (pd.Timestamp('18:00:00').time(), pd.Timestamp('23:59:59').time()),\n",
    "    'Night': (pd.Timestamp('00:00:00').time(), pd.Timestamp('05:59:59').time())\n",
    "}\n",
    "\n",
    "# Categorize listed_time into time periods\n",
    "for period, (start, end) in time_periods.items():\n",
    "    df[period] = df['listed_time'].apply(lambda x: start <= x.time() <= end)\n",
    "\n",
    "# Plot distribution of each time period\n",
    "time_period_counts = df[['Morning', 'Afternoon', 'Evening', 'Night']].sum()\n",
    "time_period_counts.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Distribution of Listings by Time Period')\n",
    "plt.xlabel('Time Period')\n",
    "plt.ylabel('Number of Listings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50205b40",
   "metadata": {},
   "source": [
    "We can observe that all of the time variables are in the standard datetime format as we addressed them in the previous notebook. From the `listed_time` column, we can observe that majority of the jobs were posted from late afternoon to evening time period. Further analysis would be desired to unveil a relationship between the time job post is listed and number of views. However, since the dataset has limited amount of information regarding time (data collected over the period of two days, in two different months, 4 days in total), we cannot draw much meaningful insights from such a limited set of time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddfbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1344d0",
   "metadata": {},
   "source": [
    "Instead, let's create dummy variables where we separate the timestamp into 'year', 'month', 'day', 'hour', 'minute' and 'second'. First, let's drop these newly created timestamp columns as they are not needed in our modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cbb444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original_listed_time, expiry, and listed_time columns\n",
    "df.drop(['Morning', 'Afternoon', 'Evening', 'Night'], axis=1, inplace=True)\n",
    "\n",
    "# Sanity Check\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the remaining timestamp columns to datetime format\n",
    "df['original_listed_time'] = pd.to_datetime(df['original_listed_time'])\n",
    "df['expiry'] = pd.to_datetime(df['expiry'])\n",
    "#df['listed_time'] = pd.to_datetime(df['listed_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4b7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc1c587",
   "metadata": {},
   "source": [
    "All three time variables are now in the datetime format. We will proceed with extracting year, month, day, hour, minute, and second into separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c5d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month, day, hour, minute, and second into separate columns\n",
    "df['original_listed_year'] = df['original_listed_time'].dt.year\n",
    "df['original_listed_month'] = df['original_listed_time'].dt.month\n",
    "df['original_listed_day'] = df['original_listed_time'].dt.day\n",
    "df['original_listed_hour'] = df['original_listed_time'].dt.hour\n",
    "df['original_listed_minute'] = df['original_listed_time'].dt.minute\n",
    "df['original_listed_second'] = df['original_listed_time'].dt.second\n",
    "\n",
    "df['expiry_year'] = df['expiry'].dt.year\n",
    "df['expiry_month'] = df['expiry'].dt.month\n",
    "df['expiry_day'] = df['expiry'].dt.day\n",
    "df['expiry_hour'] = df['expiry'].dt.hour\n",
    "df['expiry_minute'] = df['expiry'].dt.minute\n",
    "df['expiry_second'] = df['expiry'].dt.second\n",
    "\n",
    "df['listed_year'] = df['listed_time'].dt.year\n",
    "df['listed_month'] = df['listed_time'].dt.month\n",
    "df['listed_day'] = df['listed_time'].dt.day\n",
    "df['listed_hour'] = df['listed_time'].dt.hour\n",
    "df['listed_minute'] = df['listed_time'].dt.minute\n",
    "df['listed_second'] = df['listed_time'].dt.second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b180cc",
   "metadata": {},
   "source": [
    "Let's check if all the time variables have been broken down into year, month, day, hour, minute and second dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b44aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82303bec",
   "metadata": {},
   "source": [
    "All the time elements have been successfully extracted and sorted into columns respective to their time variables. Now we can drop the three original time variables: `original_listed_time`, `expiry` and `listed_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7b9f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop the original_listed_time, expiry, and listed_time columns\n",
    "df.drop(['original_listed_time', 'expiry', 'listed_time'], axis=1, inplace=True)\n",
    "\n",
    "# Sanity Check\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eea567",
   "metadata": {},
   "source": [
    "Save the preprocessed dataframe for the future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec51fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed DataFrame to a CSV file in the 'Data' folder\n",
    "df.to_csv(\"../Data/preprocessed_linkedin_job_posting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f6813",
   "metadata": {},
   "source": [
    "The three time variables have been dropped. Now we will apply one-hot encoding method to introduce dummy variables that can replace the leftover categorical variables and turn them into numerical datatype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2babb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df.select_dtypes('object')).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c61a55",
   "metadata": {},
   "source": [
    "From above, we can observe that several new dummy variables have been introduced, a column for each category in each feature. The column size has increased from 35 to 52577, and this is mostly due to `title` and `description` as these variables hold a lot of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f88cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d947842",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0bf84e",
   "metadata": {},
   "source": [
    "The `title` and `description` columns together account for 52557 (22404 + 30153) features out of 52577 newly created dummy variables. Since these two text-heavy variables are flooding the number of columns, we will conduct baseline modeling without these two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d56e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop title and description columns\n",
    "df.drop(['title', 'description'], axis=1, inplace=True)\n",
    "\n",
    "# Sanity Check\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2db5f",
   "metadata": {},
   "source": [
    "Now we can regenerate our dataframe with the dummy variables included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43c1b1",
   "metadata": {},
   "source": [
    "We can see that now we only have 50 columns instead of the previous 52577."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f54fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05495541",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3e925",
   "metadata": {},
   "source": [
    "The preprocessing has been completed. We can now proceed to baseline modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a623b514",
   "metadata": {},
   "source": [
    "<a id = 'Preprocess'><a/>   \n",
    "## Basic Model Preprocessing\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9797a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import f_oneway\n",
    "from scipy import stats\n",
    "import random\n",
    "from itertools import combinations\n",
    "from scipy.stats import kruskal\n",
    "import scikit_posthocs as sp\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd4e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting categorical columns to dummy variables\n",
    "dummy_df = pd.get_dummies(df, drop_first=True)\n",
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0645968b",
   "metadata": {},
   "source": [
    "Here, we have used 'drop_first=True' command to avoid multicollinearity, dropping first three dummy variables generated from 'pd.get_dummies' from `pay_period`, `formatted_work_type`, and `formatted_experience_level`. Let's create a heat map to observe the correlations among different job attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = dummy_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagonal creation\n",
    "diagonal_corr_matrix = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "diagonal_corr_matrix = diagonal_corr_matrix.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837821cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "plt.figure(figsize=(20, 20))\n",
    "mask = np.triu(np.ones_like(diagonal_corr_matrix, dtype=bool))\n",
    "sns.heatmap(diagonal_corr_matrix, annot=False, cmap='coolwarm', cbar=True, mask=~mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60cbba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89adcd7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter for correlations above 0.5\n",
    "high_corr_pairs = diagonal_corr_matrix[abs(diagonal_corr_matrix) > 0.5]\n",
    "\n",
    "# Iterate through the high correlation pairs and print them\n",
    "for column1 in high_corr_pairs.columns:\n",
    "    for column2 in high_corr_pairs.index:\n",
    "        correlation = high_corr_pairs[column1][column2]\n",
    "        if not np.isnan(correlation):\n",
    "            print(f\"Pair: {column1} - {column2}, Correlation: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdba9b0",
   "metadata": {},
   "source": [
    "There are several columns which has high collinearity. We have to conduct feature selection while doing linear regression. Hence, for now we will stick with vanilla baseline modeling with linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the dependent and independent variable X and y\n",
    "X = dummy_df.drop(columns=['views'])\n",
    "y = dummy_df['views']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f264aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b425e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ab54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca9c8b",
   "metadata": {},
   "source": [
    "<a id = 'Linear'><a/>   \n",
    "## Linear Regression\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ca97d",
   "metadata": {},
   "source": [
    "Prior to running a linear regression, we will remove pairs of columns that reported high collinearity to find the best scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf30c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Function to train and evaluate linear regression with column removal\n",
    "def evaluate_model(X_train, X_test, y_train, y_test, columns_to_remove):\n",
    "    \n",
    "    # Removing specified columns\n",
    "    X_train_filtered = X_train.drop(columns=[col for col in columns_to_remove if col in X_train.columns])\n",
    "    X_test_filtered = X_test.drop(columns=[col for col in columns_to_remove if col in X_test.columns])\n",
    "\n",
    "    #Scalling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_filtered)\n",
    "    X_test_scaled = scaler.transform(X_test_filtered)\n",
    "\n",
    "    # Fitting a linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions on training and test sets\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate MAE for training and test sets\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    return mae_train, mae_test\n",
    "\n",
    "\n",
    "# List of numerical columns for evaluation (These are the highly correlated columns.)\n",
    "numerical_cols = ['applies', 'application_portal', 'reposted', 'application_method',\n",
    "                  'original_listed_day', 'original_listed_month', 'expiry_month', \n",
    "                   'expiry_year', 'expiry_day', 'expiry_second', 'expiry_minute',\n",
    "                  'listed_month', 'listed_day', 'listed_hour', 'original_listed_hour',\n",
    "                  'original_listed_minute', 'listed_minute', 'listed_second', 'pay_period_YEARLY',\n",
    "                  'original_listed_second', 'med_salary', 'pay_period_Not Specified',\n",
    "                  'formatted_work_type_Part-time',\n",
    "                  'formatted_work_type_Full-time']\n",
    "\n",
    "# Initialize variables to keep track of the best model and its performance\n",
    "best_model = None\n",
    "best_mae_test = float('inf')\n",
    "best_columns_removed = None\n",
    "\n",
    "# Iterating over all possible combinations of columns to remove\n",
    "for num_columns_to_remove in range(1, len(numerical_cols) + 1):\n",
    "    for columns_to_remove_combination in combinations(numerical_cols, num_columns_to_remove):\n",
    "        mae_train, mae_test = evaluate_model(X_train, X_test, y_train, y_test, columns_to_remove_combination)\n",
    "\n",
    "        # Print MAPE and MAE scores for each combination\n",
    "        print(f\"Columns Removed: {columns_to_remove_combination}\")\n",
    "        print(f\"MAE Train: {mae_train}\")\n",
    "        print(f\"MAE Test: {mae_test}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c1642",
   "metadata": {},
   "source": [
    "Columns Removed: ('application_method', 'listed_hour')  \n",
    "MAE Train: 25.699548754830374  \n",
    "MAE Test: 25.745284190220932  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d9daf",
   "metadata": {},
   "source": [
    "Above is one of the combinations where the MAE test is lowest and difference between MAE test and train is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ce163",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = dummy_df.drop(columns=[\"views\", 'application_method', 'listed_hour'])\n",
    "y2 = dummy_df['views']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5197281",
   "metadata": {},
   "source": [
    "Before we run the model, let's scale the test and train set first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970219c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled2 = scaler.fit_transform(X_train2)\n",
    "X_test_scaled2 = scaler.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ffc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Training the model on the training data\n",
    "model.fit(X_train_scaled2, y_train2)\n",
    "\n",
    "# Making predictions on the testing set\n",
    "y_pred_test2 = model.predict(X_test_scaled2)\n",
    "\n",
    "# Making predictions on the training set\n",
    "y_pred_train2 = model.predict(X_train_scaled2)\n",
    "\n",
    "# Evaluating the model using MAE y_pred_train\n",
    "mae_train2 = mean_absolute_error(y_train2, y_pred_train2)\n",
    "print(f\"MAE TRAIN: {mae_train2}\")\n",
    "\n",
    "# Evaluating the model using MAE y_pred_test\n",
    "mae_test2 = mean_absolute_error(y_test2, y_pred_test2)\n",
    "print(f\"MAE TEST: {mae_test2}\")\n",
    "\n",
    "# Calculating R-squared\n",
    "r2 = r2_score(y_train2, y_pred_train2)\n",
    "print(f\"R-squared (R^2) value: {r2}\")\n",
    "\n",
    "# Calculating R-squared\n",
    "r2 = r2_score(y_test2, y_pred_test2)\n",
    "print(f\"R-squared (R^2) value: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6581a77c",
   "metadata": {},
   "source": [
    "The MAE on the training set is less than the MAE on the test set (25.7453 > 25.6995), hence it is not overfitted. However, the MAE score itself seems quite high. We will conduct further analysis to determine if the linear regression is ideal model to implement.\n",
    "\n",
    "The R squared value looks good as it is greater than 0.5 and closer to 1 (0.7133). The train R squared is slightly higher than the test R squared, hence there is a possibility of an overfitting in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f866f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the best fit line\n",
    "def plot_best_fit_line(y_true, y_pred, ax, label):\n",
    "    slope, intercept = np.polyfit(y_true, y_pred, 1)\n",
    "    line = slope * y_true + intercept\n",
    "    ax.plot(y_true, line, label=label, color='blue')\n",
    "\n",
    "# Plotting the scatter plot for training set\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train2, y_pred_train2, alpha=0.5, label='Data Points')\n",
    "plot_best_fit_line(y_train2, y_pred_train2, plt.gca(), 'Best Fit Line')\n",
    "plt.plot([y_train2.min(), y_train2.max()], [y_train2.min(), y_train2.max()], linestyle='--', color='red', label='Ideal Fit Line')\n",
    "plt.title('Actual vs Predicted - Training Set')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the scatter plot for test set\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test2, y_pred_test2, alpha=0.5, label='Data Points')\n",
    "plot_best_fit_line(y_test2, y_pred_test2, plt.gca(), 'Best Fit Line')\n",
    "plt.plot([y_test2.min(), y_test2.max()], [y_test2.min(), y_test2.max()], linestyle='--', color='red', label='Ideal Fit Line')\n",
    "plt.title('Actual vs Predicted - Test Set')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13346fac",
   "metadata": {},
   "source": [
    "The plot does not show a strong linear relationship along the diagonal. Ideally, the closer our best fit line is to the ideal fit line, higher the chance of predicted values matching the actual values. From the test set, we can observe that the data points tend to deviate from the accurate predictions as the ideal fit line progresses to the right. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683113e9",
   "metadata": {},
   "source": [
    "Let's plot the residuals now and draw more insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c55f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals\n",
    "residuals = y_test2 - y_pred_test2\n",
    "# Scatter plot of predicted values against residuals\n",
    "plt.scatter(y_pred_test2, residuals)\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=2)  # Adding a horizontal line at y=0 for reference\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8b8a8",
   "metadata": {},
   "source": [
    "The plot above displays heteroscedasticity, violating the assumption of homoscedasticity. Let's visualize Q-Q plot to further investigate residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a24dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Q-Q plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "stats.probplot(residuals, plot=sns.mpl.pyplot)\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1e3a43",
   "metadata": {},
   "source": [
    "The Q-Q plot above suggests that there might be some outliers in the dataset that are not clearly captured by the normal distribution. The tails of the distribution of the residuals seem to extend beyond what would be expected from a normal distribution.The S-shape of the plot suggests a departure from the normal distribution as the normal distribution has lighter tails, tails that mostly lie on the red line. Furthermore, the deviations from linearity in the QQ-plot indicate that the tails of the observed distribution are fatter than the tails of a normal distribution. Hence, there is excess kurtosis compared to a normal distribution, hinting that there is a heavy degree of risk due to residuals deviating from the perfect normal distribution line (red line)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a6e7e",
   "metadata": {},
   "source": [
    "Let's plot the distribution of residuals to show excess kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac093550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram with kernel density estimate\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Histogram of Residuals with Kernel Density Estimate')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d4579",
   "metadata": {},
   "source": [
    "The right tail is fatter than the left tail, implicating that there is an excess kurtosis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the coefficients and intercept\n",
    "coefficients = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "# Displaying the coefficients and intercept\n",
    "print(\"Intercept:\", intercept)\n",
    "for feature, coef in zip(X_train2.columns, coefficients):\n",
    "    print(f\"{feature}: {coef:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157c504",
   "metadata": {},
   "source": [
    "The coefficients provide information regarding the direction and magnitude of the relationship between each feature and our target variable, `views`. Positive coefficients suggest a positive impact on the number of views, while the negative coefficients suggest a negative impact on the number of views. The magnitude of the coefficient represents the estimated change in the number of views for one-unit change in the corresponding feature. To improve the performance of the model, we can look into some regularization techniques like Ridge and Lasso. Cross validation is also useful to get a more robust estimate of the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b528dd",
   "metadata": {},
   "source": [
    "From the observations above, we can conclude that the linear model isn't the best option. Hence, we will build a basic random forest and XGboost regressor models and observe their performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b95b51",
   "metadata": {},
   "source": [
    "<a id = 'Random'><a/>   \n",
    "## Random Forest \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Training the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the testing set\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Making predictions on the training set\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "# Calculating MAE for training set\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "print(f\"Mean Absolute Error (MAE) for TRAIN set: {mae_train}\")\n",
    "\n",
    "# Calculating MAE for test set\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "print(f\"Mean Absolute Error (MAE) for TEST set: {mae_test}\")\n",
    "\n",
    "# Calculating R-squared\n",
    "r2 = r2_score(y_train, y_pred_train)\n",
    "print(f\"R-squared (R^2) value TRAIN: {r2}\")\n",
    "\n",
    "# Calculating R-squared\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(f\"R-squared (R^2) value TEST: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935930c0",
   "metadata": {},
   "source": [
    "The MAE on the training set is significantly lower than the MAE on the test set (7.4926 < 19.6510), resulting the model to be overfitted. Moreover, the R squared values are pretty high (closer to 1.0), but the train R squared value is greater than test R squared value, indicating that there is an overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6f6df",
   "metadata": {},
   "source": [
    "Let's plot a scatter plot of actual points and predicted points for both train and test sets. We will set subsample equals to 200 so that every 200th sample is plotted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a644e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame with actual and predicted values for the training set\n",
    "train_results = pd.DataFrame({'Actual': y_train, 'Predicted': y_pred_train})\n",
    "\n",
    "# Creating a DataFrame with actual and predicted values for the test set\n",
    "test_results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_test})\n",
    "\n",
    "# Subsampling every 200th point\n",
    "subsample_factor = 200\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Creating a scatter plot for Actual values in the training set\n",
    "plt.scatter(range(0, len(train_results), subsample_factor), \n",
    "            train_results['Actual'].values[::subsample_factor], \n",
    "            label='Actual', color='blue', marker='o', alpha=0.7)\n",
    "\n",
    "# Creating a scatter plot for Predicted values in the training set\n",
    "plt.scatter(range(0, len(train_results), subsample_factor), \n",
    "            train_results['Predicted'].values[::subsample_factor], \n",
    "            label='Predicted', color='orange', marker='x', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Values - Training Set')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "# Creating a scatter plot for Actual values in the testing set\n",
    "plt.scatter(range(0, len(test_results), subsample_factor), \n",
    "            test_results['Actual'].values[::subsample_factor], \n",
    "            label='Actual', color='blue', marker='o', alpha=0.7)\n",
    "\n",
    "# Creating a scatter plot for Predicted values in the testing set\n",
    "plt.scatter(range(0, len(test_results), subsample_factor), \n",
    "            test_results['Predicted'].values[::subsample_factor], \n",
    "            label='Predicted', color='orange', marker='x', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Values - Testing Set')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f188657",
   "metadata": {},
   "source": [
    "Now let's plot the actual vs predicted with the best fit and ideal fit lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f57f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the best fit line\n",
    "def plot_best_fit_line(y_true, y_pred, ax, label):\n",
    "    slope, intercept = np.polyfit(y_true, y_pred, 1)\n",
    "    line = slope * y_true + intercept\n",
    "    ax.plot(y_true, line, label=label, color='blue')\n",
    "\n",
    "# Plotting the scatter plot for training set\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train, y_pred_train, alpha=0.5, label='Data Points')\n",
    "plot_best_fit_line(y_train, y_pred_train, plt.gca(), 'Best Fit Line')\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], linestyle='--', color='red', label='Ideal Fit Line')\n",
    "plt.title('Actual vs Predicted - Training Set')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the scatter plot for test set\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.5, label='Data Points')\n",
    "plot_best_fit_line(y_test, y_pred_test, plt.gca(), 'Best Fit Line')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], linestyle='--', color='red', label='Ideal Fit Line')\n",
    "plt.title('Actual vs Predicted - Test Set')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b40c71",
   "metadata": {},
   "source": [
    "The data points on the training set seems to be fairly close to the ideal fit line (accurate predictions). However, in the test set, as the line progresses right, the best fit line starts to deviate from the ideal fit line, and this difference suggests that the model is overfitting. Therefore, this model also needs further investigation to improve the overall performance. This can be achieved by addressing the identified limitations above, exploring feature selection techniques, and introducing more sophisticated modeling approaches to improve the predictive performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457a870",
   "metadata": {},
   "source": [
    "Let's take a look at the XGBoost regressor model and observe the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5866501",
   "metadata": {},
   "source": [
    "<a id = 'XG'><a/>   \n",
    "## XGBoost\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c187c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an XGBoost regressor\n",
    "xgb_model = XGBRegressor()\n",
    "\n",
    "# Fit the model on the training data\n",
    "xgb_model.fit(X_train, y_train, eval_metric='mae')\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_pred_train_xgb= xgb_model.predict(X_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# MAE Calculations\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train_xgb)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test_xgb)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae_train}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_test}\")\n",
    "\n",
    "# Calculating R-squared\n",
    "r2 = r2_score(y_train, y_pred_train_xgb)\n",
    "print(f\"R-squared (R^2) value TRAIN: {r2}\")\n",
    "\n",
    "# Calculating R-squared\n",
    "r2 = r2_score(y_test, y_pred_test_xgb)\n",
    "print(f\"R-squared (R^2) value TEST: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e09e4d",
   "metadata": {},
   "source": [
    "The MAE on the training set is significantly lower than the MAE on the test set (13.1894 < 20.6489), resulting the model to be overfitted. Moreover, the R squared values are pretty high (closer to 1.0), but the train R squared value is greater than test R squared value, indicating that there is an overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a9fd1",
   "metadata": {},
   "source": [
    "Let's plot a scatter plot of actual points and predicted points for both train and test sets. We will set subsample equals to 200 so that every 200th sample is plotted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame with actual and predicted values for the training set\n",
    "train_results = pd.DataFrame({'Actual': y_train, 'Predicted': y_pred_train_xgb})\n",
    "\n",
    "# Creating a DataFrame with actual and predicted values for the test set\n",
    "test_results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_test_xgb})\n",
    "\n",
    "# Subsampling every 200th point\n",
    "subsample_factor = 200\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Creating a scatter plot for Actual values in the training set\n",
    "plt.scatter(range(0, len(train_results), subsample_factor), \n",
    "            train_results['Actual'].values[::subsample_factor], \n",
    "            label='Actual', color='blue', marker='o', alpha=0.7)\n",
    "\n",
    "# Creating a scatter plot for Predicted values in the training set\n",
    "plt.scatter(range(0, len(train_results), subsample_factor), \n",
    "            train_results['Predicted'].values[::subsample_factor], \n",
    "            label='Predicted', color='orange', marker='x', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Values - Training Set')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "# Creating a scatter plot for Actual values in the testing set\n",
    "plt.scatter(range(0, len(test_results), subsample_factor), \n",
    "            test_results['Actual'].values[::subsample_factor], \n",
    "            label='Actual', color='blue', marker='o', alpha=0.7)\n",
    "\n",
    "# Creating a scatter plot for Predicted values in the testing set\n",
    "plt.scatter(range(0, len(test_results), subsample_factor), \n",
    "            test_results['Predicted'].values[::subsample_factor], \n",
    "            label='Predicted', color='orange', marker='x', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Values - Testing Set')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1365fd68",
   "metadata": {},
   "source": [
    "Now let's plot the actual vs predicted with the best fit and ideal fit lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the best fit line\n",
    "def plot_best_fit_line(y_true, y_pred, ax, label):\n",
    "    slope, intercept = np.polyfit(y_true, y_pred, 1)\n",
    "    line = slope * y_true + intercept\n",
    "    ax.plot(y_true, line, label=label, color='blue')\n",
    "\n",
    "# Plotting the scatter plot for training set\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train, y_pred_train_xgb, alpha=0.5, label='Data Points')\n",
    "plot_best_fit_line(y_train, y_pred_train_xgb, plt.gca(), 'Best Fit Line')\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], linestyle='--', color='red', label='Ideal Fit Line')\n",
    "plt.title('Actual vs Predicted - Training Set')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the scatter plot for test set\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_pred_test_xgb, alpha=0.5, label='Data Points')\n",
    "plot_best_fit_line(y_test, y_pred_test_xgb, plt.gca(), 'Best Fit Line')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], linestyle='--', color='red', label='Ideal Fit Line')\n",
    "plt.title('Actual vs Predicted - Test Set')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65824127",
   "metadata": {},
   "source": [
    "The data points on the training set seems to be fairly close to the ideal fit line (accurate predictions). However, in the test set, as the line progresses right, the best fit line starts to deviate from the ideal fit line, and this difference suggests that the model is overfitting. Therefore, this model also needs further investigation to improve the overall performance. This can be achieved by addressing the identified limitations above, exploring feature selection techniques, and introducing more sophisticated modeling approaches to improve the predictive performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232877b4",
   "metadata": {},
   "source": [
    "Here's the performance comparison of each model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95650cd",
   "metadata": {},
   "source": [
    "| Model                   | MAE (Train) | MAE (Test) | R-squared (Train) | R-squared (Test) |\n",
    "|-------------------------|-------------|------------|-------------------|------------------|\n",
    "| Linear Regression       | 25.70       | 25.75      | 0.7391            | 0.7126           |\n",
    "| Random Forest Regressor | 7.49        | 19.72      | 0.9633            | 0.7674           |\n",
    "| XGBoost Regressor       | 13.19       | 20.65      | 0.9594            | 0.7116           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f70026",
   "metadata": {},
   "source": [
    "From the table, the Random Forest regressor model seems to be the best model, we will conduct further analysis in the next notebook to confirm whether this is true or not and build a predictive model from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4669a",
   "metadata": {},
   "source": [
    "<a id = 'Summary'><a/>   \n",
    "## Summary\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ad48b",
   "metadata": {},
   "source": [
    "In this notebook, we have performed advanced EDA, such as feature engineering and model preprocessing to prepare a dataset ready for baseline modeling. In the following notebook, we will delve deeper into modeling by applying advanced modeling strategies and feature selection tools to enhance the model performance. We will also introduce NLP with deep learning models to add more depth to our analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5764cf3",
   "metadata": {},
   "source": [
    "This notebook is continued on \"Part_Three_NLP_and_Advanced_Modeling.ipynb\" notebook in the \"Notebook\" file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LinkedIn_Capstone",
   "language": "python",
   "name": "linkedin_capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
